%% ------- ROZDZIAŁ 2 ------- %%

\chapter{Zastosowanie języka programowania Python w obliczeniach analitycznych}
Python jest językiem programowania wysokiego poziomu, charakteryzujący się przede wszystkim wysoką klarownością i czytelnością kodu.
Jest to język interpretowany, co w odróżnieniu od języków kompilowanych pozwala na bardzo szybkie tworzenie i testowanie kodu.
Wadą tego rozwiązania jest niestety spadek wydajności oraz zwiększone zużycie pamięci i procesora, jednak zastosowania praktyczne Pythona zazwyczaj pozwalają na poniesienie tego typu kosztów.\\

Python został stworzony w 1989 roku przez Guido van Rossum, a do dzisiaj rozwijany jest jako projekt Open Source i zarządzany przez organizację non-profit Python Software Foundation.
Jego specyficzna struktura oraz cechy takie jak dynamiczne typowanie, automatyczne zarządzanie pamięcią, przenośność, czy duża czytelność i prostota kodu, 
umożliwiają bardzo szybkie wytwarzanie i utrzymywanie aplikacji.\\

Biblioteka standardowa języka Python zawiera wiele użytecznych modułów i gotowych rozwiązań, które wspomagają szybką i efektywną implementację kodu.
Ponadto dostępny jest \textit{Python Package Index} (PyPI) - zbiór paczek zewnętrznych, tworzonych przez niezależnych programistów, dystrybuowanych na licencjach Open Source.
Dzięki takiej mnogości pakietów i modułów język Python może być wykorzystywany w wielu projektach, łącząc różne technologie i dziedziny informatyki.
Jednym z przykładów wykorzystania tego języka jest tworzenie aplikacji internetowych za pomocą frameworku Django.
Łączy on ze sobą różne technologie wykorzystywane przy tworzeniu serwisów internetowych, zapewniając bardzo dobry mechanizm back-endowy oraz wygodne środowisko.\\

Ze względu na wyżej wymienione cechy Python znalazł również zastosowanie w analityce i analizie danych, włączając w to analizę danych statystycznych i giełdowych, a także we wspomaganiu obliczeń matematycznych.


\section{Cechy charakterystyczne języka Python}
Podstawową charakterystyczną cechą języka Python jest fakt, iż nie jest on kompilowany lecz interpretowany, czyli tłumaczony do wykonywalnego kodu maszynowego lub kodu pośredniego.
Dzięki użyciu interpretera w konsoli systemowej można bezpośrednio wykonywać kod Pythona w czasie rzeczywistym.


\begin{lstlisting}
Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information. 
>>> from sklearn import datasets
>>> 
>>> iris = datasets.load_iris()
>>> digits = datasets.load_digits()
>>> digits.data
array([[  0.,   0.,   5., ...,   0.,   0.,   0.],
       [  0.,   0.,   0., ...,  10.,   0.,   0.],
       [  0.,   0.,   0., ...,  16.,   9.,   0.],
       ..., 
       [  0.,   0.,   1., ...,   6.,   0.,   0.],
       [  0.,   0.,   2., ...,  12.,   0.,   0.],
       [  0.,   0.,  10., ...,  12.,   1.,   0.]])
>>>
>>>
\end{lstlisting}

Pozwala to na bardzo szybkie testowanie niewielkich fragmentów kodu, użycia bibliotek, a także przeprowadzanie testowych obliczeń.
Jest to również doskonałe narzędzie do sprawdzania i dostosowywania środowiska, w szczególności gdy użyte zostaje symulowane środowisko - program \textit{virtualenv}, 
który instaluje wybraną wersję interpretera we wskazanym katalogu i umożliwia instalowanie bibliotek niezależnie od tych, które zainstalowane są w systemie.\\

Kolejną wartą uwagi cechą języka Python jest jego składnia. W odróżnieniu od języków takich jak na przykład Java czy C++, w Pythonie zastosowano tak zwane dynamiczne typowanie.
Oznacza to, że podczas definiowania zmiennych nie określa się ich typu. Jest to możliwe, ponieważ w języku Python każdy element, na przykład funkcja, klasa czy też struktura danych jest obiektem.
Obiekt ten ma z góry zdefiniowany typ, więc przypisanie jego referencji do konkretnej zmiennej pomaga go w ten sposób określić.
\begin{lstlisting}
  Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
  [GCC 5.4.0 20160609] on linux2
  Type "help", "copyright", "credits" or "license" for more information.
  >>> variable_one = 44
  >>> type(variable_one)
  <type 'int'>
  >>> 
  >>> variable_one = 'text'
  >>> type(variable_one)
  <type 'str'>
  >>> 
\end{lstlisting}

Jedną z najbardziej użytecznych cech Pythona jest zastosowanie elementów programowania funkcyjnego. Elementami takimi są przykładowo wyrażenia \textit{lambda}, oraz \textit{list comprehension} i \textit{dict comprehension}.\\

Wyrażenia \textit{lambda} pozwalają na stworzenie i przypisanie do zmiennej krótkiej funkcji, która jest w stanie przyjmować argumenty oraz zwracać wartości.
Znajduje to zastosowanie w przypadkach, które wymagają wielokrotnego wykorzystania danego fragmentu kodu, a użycie ich skraca znacznie ilość wypisanych poleceń.
Pozwala to uniknąć tworzenia wielu krótkich funkcji lub metod poza obecnie wykorzystywaną przestrzenią, co często wpływa pozytywnie przede wszystkim na czytelność kodu.\\

Wyrażenia \textit{list comprehension} oraz \textit{dict comprehension} wykorzystywane są do szybkiego tworzenia odpowiednio list oraz słowników.
W swojej konstrukcji zawierają pętlę \textit{For}, która iteruje po wskazanej strukturze, na przykład liście, zwracając w każdym kroku jeden jej element.
Element ten może być sprawdzony warunkiem wbudowanym w strukturę, oraz następnie zmieniony i wbudowany w nową listę lub słownik.

\begin{lstlisting}
  Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
  [GCC 5.4.0 20160609] on linux2
  Type "help", "copyright", "credits" or "license" for more information. 
  >>> test_lambda = lambda x: x+5
  >>> test_lambda(10)
  15
  >>> 
  >>> test_list = [1, 2, 3, 4, 5]
  >>> 
  >>> list_comprehension = [x+5 for x in test_list]
  >>> list_comprehension
  [6, 7, 8, 9, 10]
  >>> 
  >>> dict_comprehension = {x: x+1 for x in test_list}
  >>> dict_comprehension
  {1: 2, 2: 3, 3: 4, 4: 5, 5: 6}
  >>> 
\end{lstlisting}

Naturalnie, natura i składnia języka Python jest o wiele bardziej różnorodna, a przedstawione przykłady odzwierciedlają jedynie namiastkę jego możliwości.
Należałoby wspomnieć tutaj między innymi o posługiwaniu się choćby wbudowanymi strukturami danych, wykorzystaniu programowania orientowanego obiektowo oraz typowych dla niego elementach.
Niemniej jednak, biorąc pod uwagę temat niniejszej pracy, którym jest przedstawienie możliwości biblioteki \textit{Scikit-learn}, wyżej wymienione podstawy uzupełnione późniejszymi wyjaśnieniami powinny wystarczyć aby w pełni zrozumieć naturę problemu.




\section{Python w obliczeniach analitycznych}
Język programowania Python jest bardzo dobrym narzędziem wspomagającym obliczenia analityczne.
Cechy tego języka zapewniają skoncentrowanie się na bezpośrednim podejściu do problemu tworzenia algorytmów i modeli, minimalizując czas projektowania od podstaw skomplikowanych algorytmów pomocniczych.
Jednak największą zaletą tego języka jest dostępność wielu bibliotek z gotowymi rozwiązaniami, które mogą zostać wykorzystane do sprawnej implementacji modeli analitycznych.
Podstawowymi bibliotekami wspomagającymi przeprowadzanie obliczeń matematycznych są \textit{NumPy} i \textit{SciPy}.\\

Pierwsza z nich dostarcza przede wszystkim obiekty wielowymiarowych list oraz szereg metod i funkcji umożliwiających szybką manipulację, przetwarzanie i sortowanie.
Zawiera także zestaw metod pozwalających na przeprowadzanie podstawowych działań statystycznych i matematycznych\cite{numpy_ug}. Stosowana jest w wielu innych bibliotekach analitycznych, na przykład w pakiecie \textit{Scikit-learn}.\\

Podstawową różnicą pomiędzy obiektami \textit{array} z pakietu \textit{NumPy}, a wbudowanymi listami języka Python jest fakt, iż podczas tworzenia obiektu ustala się stały rozmiar struktury, 
a każde zwiększenie tego rozmiaru powoduje powstanie nowego obiektu i usunięcie poprzedniego.
\begin{lstlisting}
 >>> python_list = [1, 2, 3, 4]
 >>> before_append = id(python_list)
 >>> python_list.append(5)
 >>> python_list
 [1, 2, 3, 4, 5]
 >>> after_append = id(python_list)
 >>> print(before_append, after_append)
 (140635439293360, 140635439293360)
 >>> print(before_append == after_append)
 True
 >>> 
 >>> 
 >>> import numpy as np
 >>> np_array = np.zeros(shape=(1, 4))
 >>> np_array
 array([[ 0.,  0.,  0.,  0.]])
 >>> before_resize = id(np_array)
 >>> np_array = np.resize(np_array, (1, 5))
 >>> np_array
 array([[ 0.,  0.,  0.,  0.,  0.]])
 >>> after_resize = id(np_array)
 >>> print(before_resize, after_resize)
 (139910114654128, 139910001813664)
 >>> print(before_resize == after_resize)
 False
 >>> 
\end{lstlisting}

Powyższa cecha obiektów biblioteki \textit{NumPy} oznacza, że wykonywanie operacji na takich obiektach powinno być bardziej skuteczne pod względem czasu ich przeprowadzania.
Jednakże wielokrotne przebudowywanie struktury obiektu wiąże się z bardzo dużym zużyciem pamięci, dlatego polecane jest stosowanie konwersji i tworzenie obiektów dopiero w momencie, kiedy dane są skompletowane i gotowe do przetwarzania\cite{numpy_ug}.\\

Biblioteka \textit{SciPy} zbudowana jest na podstawie biblioteki \textit{NumPy} i rozszerza ją o wiele algorytmów analizy danych.
Elementami składowymi tej biblioteki są między innymi\cite{numpy_ug}:
\begin{itemize}
 \item \textbf{cluster} - algorytmy klastrowania
 \item \textbf{linalg} - algebra liniowa
 \item \textbf{signal} - przetwarzanie sygnałów
 \item \textbf{stats} - funkcje i algorytmy statystyczne\\
\end{itemize}

Funkcjonalność biblioteki jest bardzo szeroka, dzięki czemu znajduje ona zastosowanie w wielu projektach, a także jest ona częścią składową innych bibliotek analitycznych języka Python.
Przykładową metodą należącą do biblioteki \textit{stats} jest \textit{linregress}, która umożliwia przeprowadzenie regresji liniowej dla wskazanych danych.
\begin{lstlisting}
 >>> from scipy import stats
 >>> import numpy as np
 >>>
 >>> data_x = np.random.random_integers(1, 99, 10)
 >>> data_y = np.random.random_integers(1, 99, 10)
 >>> data_x
 array([72, 45, 69, 52, 93, 14, 80, 14, 13,  5])
 >>> data_y
 array([37, 90, 19,  7, 95, 89, 88, 94, 81, 19])
 >>>
 >>> slope, intercept, r_value,
     p_value, std_err = stats.linregress(data_x, data_y)
 >>> print(slope, intercept, r_value, p_value, std_err)
 (-0.033350664784966184, 63.424125380672955,
  -0.029541780200591877, 0.93543372355182242, 0.39896357644710517)
 >>> 
\end{lstlisting}

\section{Pakiet Scikit-learn}

\subsection{Cel i przeznaczenie pakietu}
Biblioteka \textit{Scikit-learn} zawiera zestaw zaawansowanych narzędzi stosujących uczenie maszynowe do analizy danych w języku Python.
Dystrybuowana jest na licencji BSD, która pozwala na modyfikowanie i rozprowadzanie kodu źródłowego, a nawet na włączanie go do produktów komercyjnych pod warunkiem umieszczenia w dokumentacji odpowiednich adnotacji dotyczących autorów.
Dzięki temu zaliczana jest do wolnego oprogramowania, które rozwijane jest przez społeczność kontrybutorów.
Większa część kodu stworzona jest bezpośrednio w języku Python, lecz niektóre elementy takie jak na przykład implementacje SVM oraz modeli liniowych oparte są na bibliotekach języka C++, odpowiednio LibSVM oraz LibLinear\cite{scikit_article}.\\

Podstawowym założeniem twórców biblioteki jest priorytetyzacja utrzymywania jakości i czytelności kodu, ponad implementację bardzo wielu funkcji\cite{scikit_article}.
Dodatkowo rozwijana jest wysokiej jakości kompleksowa dokumentacja, co razem stanowi bardzo dobrą bazę do rozwijania całego projektu przez wielu niezależnych deweloperów i wydawania stabilnych wersji produktu.
Scikit-learn bazuje na trzech bibliotekach języka Python: \textit{NumPy}, \textit{SciPy} i \textit{Matplotlib}. Stanowią one wymagania systemowe, niezbędne do poprawnego działania pakietu.\\

W pakiecie \textit{Scikit-learn} algorytmy podzielone są na algorytmy uczenia z nadzorem oraz algorytmy uczenia bez nadzoru\cite{scikit_doc}.
Pierwsze z nich opierają się o podział danych na uczące i testowe, gdzie dane uczące zawierają przykładowe oczekiwane wartości na podstawie których budowany jest model.
W zestawie algorytmów uczenia nadzorowanego znaleźć można między innymi\cite{scikit_doc}:
\begin{itemize}
 \item Ogólne modele liniowe
 \item Liniową i kwadratową analizę dyskryminacyjną
 \item Regresję grzbietową (KRR)
 \item Maszynę wektorów nośnych (SVM)
 \item Algorytm k najbliższych sąsiadów
 \item Proces Gaussa
 \item Naiwny klasyfikator Bayesa
 \item Drzewa decyzyjne\\
\end{itemize}

Algorytmy uczenia bez nadzoru w pakiecie \textit{Scikit-learn}, dla których dane uczące nie posiadają żadnych wartości odniesienia, możemy natomiast podzielić między innymi na:
\begin{itemize}
 \item Klasteryzację
 \item Estymację kowariancji
 \item Nieliniową redukcję przestrzenną\\
\end{itemize}

W niniejszej pracy zastosowane zostały algorytmy uczenia z nadzorem.

\subsection{Ogólne modele liniowe}
W pakiecie \textit{Scikit-learn} przedstawione zostały algorytmy regresji, w których oczekiwane wartości docelowe są liniową kombinacją wartości wejściowych.
Podstawę stanowi równanie regresji:
\begin{ceqn}
\begin{align}
\hat{y} = \omega_{0} + \omega_{1}X_{1} + \omega_{2}X_{2} + ... + \omega_{p}X_{k}
\end{align}
\end{ceqn}

Wektor $\omega = (\omega_{1}, ..., \omega_{p})$ jest utożsamiany z parametrem \textit{coef\_}, a wyraz wolny $\omega_{0}$ z parametrem \textit{intercept\_} \cite{scikit_doc}.\\

\newpage

Regresja liniowa w pakiecie \textit{Sciki-learn} dostępna jest poprzez obiekt \textbf{LinearRegression}.
Parametry, jakie przyjmuje ten obiekt w momencie inicjalizacji to\cite{scikit_doc}:
\begin{itemize}
 \item \textit{fit\_intercept}: boolean, optional, default True
 \item \textit{normalize}: boolean, optional, default False
 \item \textit{copy\_X}: boolean, optional, default True
 \item \textit{n\_jobs}: int, optional, default 1\\
\end{itemize}

Parametr \textit{fit\_intercept} przyjmuje wartości typu boolean, a jego domyślna wartość wynosi \textit{True}.
W takim wypadku przed procesem dopasowania modelu obliczany jest punkt przecięcia z osią \textit{y} dla modelu.
Jeśli przed procesem dopasowywania modelu dokonano normalizacji danych, parametr może zostać ustawiony wartością \textit{False}.\\

Parametr \textit{normalize} jest flagą uwzględniającą lub pomijającą wstępną normalizację danych.
Przyjmuje wartości typu boolean i domyślnie jest ustawony na \textit{False} i jest ignorowany, gdy parametr \textit{fit\_intercept} jest ustawiony wartością \textit{False}.
Jeśli przed dokonaniem dopasowania modelu nie została przeprowadzona normalizacja danych, ustawienie tego parametru na \textit{True} spowoduje ich przetworzenie.\\

Parametr \textit{copy\_X} jest odpowiedzialny za ustawienie flagi kopiowania, bądź nadpisywania wartości X. Domyślnie ustawiony jest na wartość \textit{True}, więc w tym przypadku podane dane X są kopiowane.\\

Parametr \textit{n\_jobs} określa ilość procesorów, które będą użyte do przetwarzania danych. Przyjmuje wartości typu integer, a domyślnie ustawiony jest wartością 1.
Jeśli podana zostanie wartość -1, wykorzystane zostaną wszystkie dostępne procesory. Znajduje on praktyczne zastosowanie przy przetwarzaniu dużych zbiorów danych, skracając czas wykonania obliczeń.\\

\begin{lstlisting}
 >>> from sklearn.linear_model import LinearRegression
 >>> LinearRegression()
 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1,
                  normalize=False)
 >>>
\end{lstlisting}\\


Obiekt udostępnia następujące metody:
\begin{itemize}
 \item \textit{fit}: dopasowanie modelu liniowego
 \item \textit{get\_params}: pobranie parametrów estymacji
 \item \textit{predict}: predykcja na podstawie modelu liniowego
 \item \textit{score}: współczynnik determinacji $R^2$
 \item \textit{set\_params}: ustawienie parametrów estymacji\\
\end{itemize}

\newpage

Metoda \textit{fit()} jest odpowiedzialna za dopasowanie modelu dla podanych danych. Jako argumenty przyjmuje dane testowe X, które powinny być strukturą definiowaną typem \textit{numpy.ndarray}. Przykładowo:
\begin{lstlisting}
 >>> X = np.asarray([x for x in range(5)])
 >>> X
 array([0, 1, 2, 3, 4])
 >>> X.reshape(-1, 1)
 array([[0],
        [1],
        [2],
        [3],
        [4]])
 >>>
\end{lstlisting}
Wartości y, jako kolejny argument przyjmowany przez funkcję, powinny być zgodne typem z wartościami X, lecz ich postać powinna być macierzą jednowymiarową.
Ważną informacją jest fakt, iż struktura X oraz struktura y powinny mieć dokładnie ten sam rozmiar. W innym przypadku niemożliwe jest dokonanie dopasowania, a metoda zwraca błąd \textit{ValueError}.
W przypadku danych giełdowych wartości X utożsamiane są przez przekształcone wartości dat kolejnych próbek danych testowych, natomiast wartości y to odpowiadające im ceny akcji lub wielkości wolumenu.\\

Kolejną metodą klasy \textit{LinearRegression} jest \textit{predict()}. Umożliwia ona dokonanie obliczeń predykcji dla podanych danych testowych X, na podstawie dopasowanego przy pomocy metody \textit{fit()} modelu.
Przyjmowane dane testowe powinny być zgodne zarówno typem, jak i formatem struktury z danymi testowymi X użytymi do przeprowadzenia dopasowania modelu metodą \textit{fit()}.

\begin{lstlisting}
 >>> import random
 >>> import numpy as np
 >>> from sklearn.linear_model import LinearRegression
 >>>
 >>> X = np.asarray(range(1, 10))
 >>> X.reshape(-1, 1)
 >>> y = np.asarray(sorted([random.uniform(1.0, 2.0)
                            for x in range(10)]))
 >>> print(y)
 array([ 1.03622622,  1.08102786,  1.15758493,  1.30854005,
         1.43797429,  1.47473032,  1.5770967 ,  1.64707456,
         1.7256157 ,  1.8449893 ])
 >>> X_test = np.asarray(11, 15)
 >>> X_test = X_test.reshape(-1, 1)
 >>>
 >>> linear = LinearRegression()
 >>> linear.fit(X, y)
 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1,
                  normalize=False)
 >>> linear.predict(X_test)
 array([ 2.02318035,  2.11457948,  2.20597861,  2.29737775])
 >>> linear.predict(X)
 array([ 1.0177899 ,  1.10918903,  1.20058816,  1.2919873 ,
         1.38338643,  1.47478556,  1.56618469,  1.65758382,
         1.74898295,  1.84038209])
 >>>
\end{lstlisting}


\subsection{Nieliniowe modele regresji}
W pakiecie \textit{Scikit-learn} regresja grzbietowa reprezentowana jest przez klasę \textit{KernelRidge}.
Jest ona w istocie połączeniem algorytmu regresji grzbietowej z tak zwanymi funkcjami jądra (kernel functions).
Najważniejszymi parametrami przyjmowanymi przez tą klasę są:
\begin{itemize}
 \item \textit{alpha}: {float, array}
 \item \textit{kernel}: string, default \textit{linear}
 \item \textit{gamma}: float, default None\\
\end{itemize}

Parametr \textit{kernel} może przyjmować wartości nazw funkcji jądra, na przykład: rbf, laplacian, polynomial lub linear. W niniejszej pracy we wszystkich nieliniowych modelach regresji zastosowano funkcję \textit{rbf}.\\

Parametry \textit{alpha} i \textit{gamma} dobierane są dzięki klasie \textit{sklearn.model\_selection.GridSearchCV}, 
która przyjmuje listy parametrów wraz z oryginalnym obiektem regresji i iterując po ich iloczynie kartezjańskim dobiera taką kombinację, która zwraca najlepsze dopasowanie metody \textit{fit{}}.

\begin{lstlisting}
 >>> krr = GridSearchCV(KernelRidge(kernel='rbf'),
                                param_grid={
                         "alpha": [1e0, 0.1, 1e-2, 1e-3],
                         "gamma": np.logspace(-2, 2, 5)
			 })
 >>> >>> krr
GridSearchCV(cv=None, error_score='raise',
	     estimator=KernelRidge(alpha=1, coef0=1, degree=3,
	                           gamma=None, kernel='rbf',
	                           kernel_params=None),
             fit_params={}, iid=True, n_jobs=1,
             param_grid={'alpha': [1.0, 0.1, 0.01, 0.001], 
                         'gamma': array([  1.00000e-02,
                                           1.00000e-01,
                                           1.00000e+00,
                                           1.00000e+01,
                                           1.00000e+02])},
             pre_dispatch='2*n_jobs', refit=True,
             return_train_score=True, scoring=None, verbose=0)
 >>>             
\end{lstlisting}

Pozostałymi, zastosowanymi w pracy metodami regresji nieliniowej są:
\begin{itemize}
 \item Regresja Wektorów nośnych - \textit{sklearn.svm.SVR} - przyjmująca parametry \textit{C} i \textit{epsilon}
 \item Regresja procesu Gaussa - \textit{sklearn.gaussian\_process.GaussianProcessRegressor} - przyjmująca prarmetr \textit{alpha}\\
\end{itemize}

Do przeprowadzenia procesu normalizacji danych może być użyta klasa \textit{StandardScaler} pakietu \textit{Scikit-learn}. 
\begin{lstlisting}
 >>> sc_x = StandardScaler()
 >>> sc_y = StandardScaler()

 >>> sort_dates = sc_x.fit_transform(dates_delta)
 >>> sort_values = sc_y.fit_transform(sort_values)
 
 >>> sort_dates = sc_x.inverse_transform(sort_dates)
 >>> sort_values = sc_y.inverse_transform(sort_values)
 >>>
\end{lstlisting}

Metoda \textit{fit\_transform} utworzonego obiektu klasy \textit{StandardScaler} umożliwia przeprowadzenie normalizacji, natomiast metoda \textit{inverse\_transform} umożliwia powrót do oryginalnych wartości.



